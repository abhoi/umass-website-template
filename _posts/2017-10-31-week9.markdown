---
layout: post
title: "[VRAR] ARKit + CoreLocation Navigation Critical Analysis"
date:   2017-10-31
categories: virtual-reality augmented-reality
author: abhoi
---

![Header Image]({{ site.url }}/images/vrar/week9/ar_head.jpg)

## Navigation using Augmented Reality!

For the Student's Choice Presentation portion of the CS491 - Virtual and Augmented Reality class, I would like to talk about augmented reality based navigation. Particularly, this specific application of Apple's ARKit to develop a unique navigation application using AR and iPhone's camera. The application merges CoreLocation (Apple's proprietary location service framework) and ARKit (Apple's own Augmented Reality framework).

The application was developed by [Andrew Hart](https://github.com/ProjectDent). The application cleverly uses two aspects of Apple's frameworks: ARKit and CoreLocation. It uses GPS data in addition to highly accurate AR to overlay information onto the screen. The first application of this is when you want to find a location on maps and need walking directions to the location. A video demonstrating the storyboard is as follows:

![Demonstration]({{ site.url }}/images/vrar/week9/ar2.gif)

A second great use is to highlight common buildings and city structures when panning over them. An example:

![Demonstration 2]({{ site.url }}/images/vrar/week9/ar1.gif)

## Cool! But what's the technical details? Pros vs Cons? Is it good?

First, I want to talk about the location services part of the application. How does the application get location data? How does it work with placing nodes and 3D objects in the AR space in your phone? What is the accuracy? Are there alternative methods to getting location data?

- **CLLocation:** The application gets the user's current location using Apple's [CLLocationManager](https://developer.apple.com/documentation/corelocation/cllocationmanager) class. An object of that class can be declared which can be configured further to specify **accuracy, type of authorization, delegates**, etc. The CLLocationManger.location property returns a [CLLocation](https://developer.apple.com/documentation/corelocation/cllocation) property. This CLLocation property holds the appropriate data such as **latitude, longitude, altitude**, etc. An example configuration of the CLLocationManager which I implemented is:

		locationManager.delegate = self
		locationManager.desiredAccuracy = kCLLocationAccuracyBest
		locationManager.requestAlwaysAuthorization()
		locationManager.distanceFilter = 50
		locationManager.startUpdatingLocation()

The location manager sets the current ViewController as the delegate, sets desired accuracy to best, requests authorization to access location always (even when the application is in background), sets the distance filter to 50m, and allows the location manager to update location in a set time interval.

- **Transform Matrix**: Getting the user's location is great. You get the latitude and longitude of the user's current location. However, this information is completely futile in its current form with respect to ARKit. This is because ARKit has its own **coordinate space**. This can be visualized as:

![ARKit Coordinates](https://cdn-images-1.medium.com/max/800/1*IRvOJvHSBOpLxbdGCzck-g.png)

Thus, if you move in the **-Z** direction, you move forward in the ARScene space. **+X** would move you right. **+Y** would move you up. This sets the tone that you cannot directly use latitude and longitude data in ARKit space. Imagine you want to put a sphere in a specific location but you do not know the transform location of that object, only the latitude and longtiude. How does one go about transforming the coordinate system into a the 3D space that ARKit can understand? This is where some computer graphics knowledge comes into picture. We use the concept of representing coordinate systems in terms of 4x4 **matrices**:

![Matrix](https://cdn-images-1.medium.com/max/800/1*6aCjMoJnuQov_GMoUTqwCw.png)

Using this representation, you can perform operations such as **translation**:

	[1 0 0 X]   [x]   [ x + X*w ]
	[0 1 0 Y] x [y] = [ y + Y*w ]
	[0 0 1 Z]   [z]   [ z + Z*w ]
	[0 0 0 1]   [w]   [ W       ]	

and **rotation**:

![Rotation Matrix](https://cdn-images-1.medium.com/max/800/1*FE4m9047G3ixBAvJlvYpaw.png)

You can perform SIMD (Single instruction multiple data) operations to get a transformed matrix. This should allow you to get a 4x4 matrix representation that you can assign as a node's transform property to place it into the ARKit space.

However, there is a problem. To accurately rotate an object, you need to calculate the **bearing** between the user's current location and the desired location of the place of interest. It is not easy to calculate that as the accuracy is going to take a hit. As Apple's own documentation says, ARKit transformations are an inexact science and it will take a lot of hit and miss to get the implementation just right.

- **Tree Node Structure**: How does Apple's ARKit know how to structure nodes in your ARScene? This video visually and clearly explains how this works:

[![ARKit Tutorial](http://img.youtube.com/vi/tgPV_cRf2hA/0.jpg)](https://youtu.be/tgPV_cRf2hA?t=1256)

I hope that clearly explains node structure and how to work in that aspect.

- **How good is the accuracy?**: I think the following video gives some intuition as to what the problem is:

[![Accuracy Problem](http://img.youtube.com/vi/6Lo0Z7CkMWw/0.jpg)](https://www.youtube.com/watch?v=6Lo0Z7CkMWw)

The accuracy is **NOT** that good. There is an error of upto **10-20 meters** at a give time depending on the location. This can clearly result in errors depending on the type of location. This is the biggest problem with this idea. ARKit has an error of between a **few meters to a few millimeters**. But a CLLocation has an error of **10-20 meters**! 

- **Alternative Methods to Get Location**: For the user's current location, it is reccommended to use CLLocation as it provides a pretty good estimate for iPhone location. To get Places location and information, you can possibly use [Google Maps/Places SDK](https://developers.google.com/maps/documentation/ios-sdk/) or [Mapbox SDK](https://www.mapbox.com/ios-sdk/). You can definitely use Google Maps to get interesting places or locations but for current user location, it is more accurate to use Apple's CoreLocation framework.

![Google Maps](https://www.programmableweb.com/wp-content/Google-Maps-for-iOs.jpg)

![Mapbox](https://www.mapbox.com/ios-sdk/api/3.6.4/img/screenshot.png)

## PROS

1. **Accurately places SCNNodes using MKRouteStep (MapKit) to display path to a destination:** The nodes are placed accurately wherever needed and if the location accuracy is good, the nodes are placed accurately. This placement is also very easy to manage because of the root tree structure.
2. **Can search and update view to get new destination with new path:** The user can always set up a new destination to traverse towards and the application resets the view to display the new nodes.
3. **Range of depth from camera view is good and can judge depth well:** The camera judges depth really well unless there are too many dynamic objects in the camera frame.
4. **Almost zero latency due to VIO:** In my use, there was minimal delay or latency.
5. **Mapping of objects is accurate (only when distinguishing features are available):** AR devices usually fail to map objects when they cannot find objects to create the ARScene. ARKit handles it really well (unless there is a unicolor plane infront of it).

## CONS

1. **Location accuracy suffers when the distance is less than 10 meters:** Location accuracy suffers due to the limitations of Google Maps and CoreLocation. This is more of a framework problem really.
2. **True North calibration is a problem (actually it is more an Apple problem than developer problem):** This problem is everywhere as the phone cannot accurately calibrate the ARScene to True North all the time. This requires resetting the application a couple of times. (This problem is specific to location based applications)
3. **Distance updates require complex calculation that may lag if the distances are too far apart:** Recalculating node location due to changes in phone location can result in many matrix transformation applications. This exponentially increases with the number of nodes in the scene. However, Apple tries to ease the problem using SIMD calculations.
4. **SCNNodes that need to be updated nearby can cause overlap that needs to be taken care of:** Node overlapping rarely happens but it may happen when the destination is too close to the current location.
5. **On some occasions, SCNNode placement fails in terms of depth:** Depth detection, again, is an inexact science and requires development by the framework developers rather than application developers.

## Other Implementations Using the same idea

This project is not the only one doing ARKit + CoreLocation. Some others are:

- **UrbanBird**: [![UrbanBird](http://img.youtube.com/vi/QaJAOtvzxCE/0.jpg)](https://www.youtube.com/watch?v=QaJAOtvzxCE)

- **LocateAR**: [![LocateAR](http://img.youtube.com/vi/FrQ4Oqzdi0Y/0.jpg)](https://www.youtube.com/watch?v=FrQ4Oqzdi0Y)

- **Neon**: [![Neon](http://img.youtube.com/vi/VV9jMjncj2I/0.jpg)](https://www.youtube.com/watch?v=VV9jMjncj2I)

## Conclusion

There are lots of implementations floating around the internet with the same concept. However, until the problem of location accuracy is solved, this implementation will remain as a proof of concept and not develop into a marketable/consumer level application.

## References:

[9to5mac](https://9to5mac.com/2017/07/21/arkit-augmented-reality-navigation/)

[Github Project](https://github.com/ProjectDent/ARKit-CoreLocation)

[Twitter Demonstration](https://twitter.com/AndrewProjDent/status/888380207962443777?ref_src=twsrc%5Etfw&ref_url=https%3A%2F%2F9to5mac.com%2F2017%2F07%2F21%2Farkit-augmented-reality-navigation%2F)

[Yat Choi](https://medium.com/@yatchoi/getting-started-with-arkit-real-life-waypoints-1707e3cb1da2)

[Christopher Webb-Orenstein](https://medium.com/journey-of-one-thousand-apps/arkit-and-corelocation-part-one-fc7cb2fa0150)
